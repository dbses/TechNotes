# 22 | 想成为架构师，你必须知道CAP理论

**CAP 理论**

第一版解释：

> 对于一个分布式计算系统，不可能同时满足一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三个设计约束。

第二版解释：

> 在一个分布式系统（指互相连接并共享数据的节点的集合）中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲。

第二版强调了两点：

- 互相连接和共享数据

  因为分布式系统并不一定会互联和共享数据。最简单的例如 Memcache 的集群，相互之间就没有连接和共享数据，因此 Memcache 集群这类分布式系统就不符合 CAP 理论探讨的对象；而 MySQL 集群就是互联和进行数据复制的，因此是 CAP 理论探讨的对象。

- 读写操作

  CAP 关注的是对数据的读写操作，而不是分布式系统的所有功能。例如，ZooKeeper 的选举机制就不是 CAP 探讨的对象。

相比来说，第二版的定义更加精确。

**一致性（Consistency）**

第一版解释：

> 所有节点在同一时刻都能看到相同的数据。

第二版解释：

> 对某个指定的客户端来说，读操作保证能够返回最新的写操作结果。

第一版从节点 node 的角度描述，关键词是看到，强调同一时刻拥有相同数据。

第二版从客户端 client 的角度描述，关键词是写操作。

**可用性（Availability）**

第一版解释：

> 每个请求都能得到成功或者失败的响应。

第二版解释：

> 非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）。

第一版强调了每个请求，响应分为成功和失败，

第二版强调了非故障的节点，合理的时间内返回合理的响应。

**分区容忍性（Partition Tolerance）**

第一版解释：

> 出现消息丢失或者分区错误时系统能够继续运行。

第二版解释：

> 当出现网络分区后，系统能够继续起作用。

第一版是运行，描述分区用的是消息丢失或者分区错误；

第二版是起作用，描述分区用的是网络分区；

**CAP 应用**

虽然 CAP 理论定义是三个要素中只能取两个，但放到分布式环境下来思考，我们会发现必须选择 P（分区容忍）要素。

如果我们选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证 C，系统需要禁止写入，当有写入请求时，系统返回 error（例如，当前系统不允许写入），这又和 A 冲突了，因为 A 要求返回 no error 和 no timeout。因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。

1.CP - Consistency/Partition Tolerance

如下图所示，为了保证一致性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 需要返回 Error，提示客户端 C“系统现在发生了错误”，这种处理方式违背了可用性（Availability）的要求，因此 CAP 三者只能满足 CP。

![image-20210121232648140](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210121232648.png)

2.AP - Availability/Partition Tolerance

如下图所示，为了保证可用性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 将当前自己拥有的数据 x 返回给客户端 C 了，而实际上当前最新的数据已经是 y 了，这就不满足一致性（Consistency）的要求了，因此 CAP 三者只能满足 AP。注意：这里 N2 节点返回 x，虽然不是一个“正确”的结果，但是一个“合理”的结果，因为 x 是旧的数据，并不是一个错乱的值，只是不是最新的数据而已。

![image-20210121232719802](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210121232719.png)

**思考题**

基于 Paxos 算法构建的分布式系统，属于 CAP 架构中的哪一种？

# 23 | 想成为架构师，你必须掌握的CAP细节

今天，我来讲讲CAP 的具体细节，简单对比一下 ACID、BASE 几个概念的关键区别点。

**CAP 关键细节点**

- CAP 关注的粒度是数据，而不是整个系统。

在实际设计过程中，每个系统不可能只处理一种数据，而是包含多种类型的数据，有的数据必须选择 CP，有的数据必须选择 AP。而如果我们做设计时，从整个系统的角度去选择 CP 还是 AP，就会发现顾此失彼，无论怎么做都是有问题的。

- CAP 是忽略网络延迟的。

实际情况下，从节点 A 复制数据到节点 B，总是需要花费一定时间的。如果是相同机房，耗费时间可能是几毫秒；如果是跨地域的机房，耗费的时间就可能是几十毫秒。

这就意味着，CAP 理论中的 C 在实践中是不可能完美实现的，在数据复制的过程中，节点 A 和节点 B 的数据并不一致。对于某些严苛的业务场景，例如和金钱相关的用户余额，或者和抢购相关的商品库存，技术上是无法做到分布式场景下完美的一致性的，只能选择 CA。

但系统整体还是可以应用分布式架构的。例如，下面的架构图是常见的将用户分区的分布式架构。

![image-20210123224553959](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210123224554.png)

对于单个用户来说，读写操作都只能在某个节点上进行；对所有用户来说，有一部分用户的读写操作在 Node 1 上，有一部分用户的读写操作在 Node 2 上。

- 放弃并不等于什么都不做，需要为分区恢复后做准备。

分区期间放弃 C 或者 A，并不意味着永远放弃 C 和 A，我们可以在分区期间进行一些操作，从而让分区故障解决后，系统能够重新达到 CA 的状态。

以用户管理系统为例，对于用户账号数据，假设我们选择了 CP，则分区发生后，节点 1 可以继续注册新用户，节点 2 无法注册新用户，此时节点 1 可以将新注册但未同步到节点 2 的用户记录到日志中。当分区恢复后，节点 1 读取日志中的记录，同步给节点 2，当同步完成后，节点 1 和节点 2 就达到了同时满足 CA 的状态。

对于用户信息数据，假设我们选择了 AP，则分区发生后，节点 1 和节点 2 都可以修改用户信息，但两边可能修改不一样。当分区恢复后，系统按照某个规则来合并数据。例如，按照“最后修改优先规则”，按照“字数最多优先规则”，也可以完全将数据冲突报告出来，由人工来选择具体应该采用哪一条。

**ACID**

ACID 是数据库管理系统为了保证事务的正确性而提出来的一个理论，ACID 包含四个约束。

- Atomicity（原子性）

一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。

- Consistency（一致性）

在事务开始之前和事务结束以后，数据库的完整性没有被破坏。

- Isolation（隔离性）

数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。

- Durability（持久性）

事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

ACID 中的 C 和 CAP 中的 C 含义完全不一样。ACID 中的 C 是指数据库的数据完整性，而 CAP 中的 C 是指分布式节点中的数据一致性。

**BASE**

BASE 是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency），核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性。

- 基本可用（Basically Available）

分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。

- 软状态（Soft State）

允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是 CAP 理论中的数据不一致。

- 最终一致性（Eventual Consistency）

系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。“一定时间”和数据的特性是强关联的，不同的数据能够容忍的不一致时间是不同的。

BASE 理论本质上是对 CAP 中 AP 方案的一个补充。

- CAP 理论是忽略延时的，而实际应用中延时是无法避免的。
- AP 方案中牺牲一致性只是指分区期间，而不是永远放弃一致性。

**思考题**

假如你来设计电商网站的高可用系统，按照 CAP 理论的要求，你会如何设计？

答：一个电商网站核心模块有会员，订单，商品，支付，促销管理等。

对于会员模块，包括登录，个人设置，个人订单，购物车，收藏夹等，这些模块保证AP，数据短时间不一致不影响使用。

订单模块的下单付款扣减库存操作是整个系统的核心，需要保证 CA，在极端情况下牺牲P是可以的。

商品模块的商品上下架和库存管理保证CP。搜索功能因为本身就不是实时性非常高的模块，所以保证AP就可以了。

促销是短时间的数据不一致，结果就是优惠信息看不到，但是已有的优惠要保证可用，而且优惠可以提前预计算，所以可以保证AP。

# 24 | FMEA方法，排除架构可用性隐患的利器

我们在进行架构设计的时候必须全面分析系统的可用性，那么如何才能做到“全面”呢？

**FMEA 介绍**

FMEA 是一套分析和思考的方法，而不是某个领域的技能或者工具。FMEA 并不能指导我们如何做架构设计，而是当我们设计出一个架构后，再使用 FMEA 对这个架构进行分析，看看架构是否还存在某些可用性的隐患。

**FMEA 方法**

FMEA 分析的方法其实很简单，就是一个 FMEA 分析表，常见的 FMEA 分析表格包含下面部分。

1. 功能点

注意这里的“功能点”指的是从用户角度来看的，而不是从系统各个模块功能点划分来看的。

2. 故障模式

故障模式指的是系统会出现什么样的故障，包括故障点和故障形式。例如 MySQL 响应时间达到 3 秒。

3. 故障影响

当发生故障模式中描述的故障时，功能点具体会受到什么影响。

4. 严重程度

严重程度指站在业务的角度故障的影响程度，一般分为“致命 / 高 / 中 / 低 / 无”五个档次。严重程度 = 功能点重要程度 × 故障影响范围 × 功能点受损程度。

5. 故障原因

为何这里还要单独将故障原因列出来呢？主要原因有这几个：

- 不同的故障原因发生概率不相同
- 不同的故障原因检测手段不一样
- 不同的故障原因的处理措施不一样

6. 故障概率

这里的概率就是指某个具体故障原因发生的概率。一般分为“高 / 中 / 低”三档即可，具体评估的时候需要有以下几点需要重点关注。

- 硬件
- 开源系统
- 自研系统

7. 风险程度

风险程度就是综合严重程度和故障概率来一起判断某个故障的最终等级，风险程度 = 严重程度 × 故障概率。

8. 已有措施

针对具体的故障原因，系统现在是否提供了某些措施来应对，包括：检测告警、容错、自恢复等。

9. 规避措施

规避措施指为了降低故障发生概率而做的一些事情，可以是技术手段，也可以是管理手段。

10. 解决措施

解决措施指为了能够解决问题而做的一些事情，一般都是技术手段。

11. 后续规划

综合前面的分析，就可以看出哪些故障我们目前还缺乏对应的措施，哪些已有措施还不够，针对这些不足的地方，再结合风险程度进行排序，给出后续的改进规划。

**FMEA 实战**

下面我以一个简单的样例来模拟一次 FMEA 分析。假设我们设计一个最简单的用户管理系统，包含登录和注册两个功能，其初始架构是：

![image-20210124225257318](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210124225257.png)

我们来看看这个架构通过 FMEA 分析后，能够有什么样的发现，下表是分析的样例：

![image-20210124231440227](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210124231831.png)

经过上表的 FMEA 分析，将“后续规划”列的内容汇总一下，我们最终得到了下面几条需要改进的措施：

- MySQL 增加备机。
- MC 从单机扩展为集群。
- MySQL 双网卡连接。

改进后的架构如下：

![image-20210124231856758](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210124231856.png)

**思考题**

请使用 FMEA 方法分析一下 HDFS 系统的架构，看看 HDFS 是如何应对各种故障的，并且分析一下 HDFS 是否存在高可用问题。

# 25 | 高可用存储架构：双机架构

存储高可用方案的本质都是通过将数据复制到多个存储设备，通过数据冗余的方式来实现高可用，对任何一个高可用存储方案，我们需要从以下几个方面去进行思考和分析：

- 数据如何复制？
- 各个节点的职责是什么？
- 如何应对复制延迟？
- 如何应对复制中断？

常见的高可用存储架构有主备、主从、主主、集群、分区。

**主备复制**

主备方案结构图如下：

![image-20210125212015378](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210125212015.png)

主备架构中的“备机”主要还是起到一个备份作用，并不承担实际的业务读写操作，如果要把备机改为主机，需要人工操作。

优点：

- 对于客户端来说，不需要感知备机的存在；
- 对于主机和备机来说，双方只需要进行数据复制即可，无须进行状态判断和主备切换这类复杂的操作。

缺点：

- 备机仅仅只为备份，并没有提供读写操作，硬件成本上有浪费；
- 故障后需要人工干预，无法自动恢复。

**主从复制**

主机负责读写操作，从机只负责读操作，不负责写操作。主从复制架构图如下：

![image-20210125212335788](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210125212335.png)

优点：

- 主从复制在主机故障时，读操作相关的业务可以继续运行。
- 主从复制架构的从机提供读操作，发挥了硬件的性能。

缺点：

- 主从复制架构中，客户端需要感知主从关系，复杂度比主备复制要高。
- 会出现主从复制延迟问题。
- 故障时需要人工干预。

**双机切换**

双机切换是为了解决以下这两个问题：

- 主机故障后，无法进行写操作；
- 如果主机无法恢复，需要人工指定新的主机角色。

要实现一个完善的切换方案，必须考虑这几个关键的设计点：

- 主备间状态判断

  主要包括两方面：状态传递的渠道（是相互间互相连接，还是第三方仲裁？），以及状态检测的内容（例如机器是否掉电、进程是否存在、响应是否缓慢等）。

- 切换决策

  主要包括几方面：切换时机、切换策略、自动程度。

- 数据冲突解决

  当原有故障的主机恢复后，新旧主机之间可能存在数据冲突。

**双机切换常见架构**

1. 互连式

互连式就是指主备机直接建立状态传递的渠道，架构图如下：

![image-20210125222218742](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210125222218.png)

客户端同时记录主备机的地址，哪个能访问就访问哪个。互连式主备切换主要的缺点在于：

- 如果状态传递的通道本身有故障，那么备机也会认为主机故障了从而将自己升级为主机，而此时主机并没有故障，最终就可能出现两个主机。
- 如果为了解决上个问题使用多个通道，则后续的状态决策会更加复杂，因为对备机来说，可能从不同的通道收到了不同甚至矛盾的状态信息。

2. 中介式

中介式指的是在主备两者之外引入第三方中介，主备机之间不直接连接，而都去连接中介，并且通过中介来传递状态信息，其架构图如下：

![image-20210125223048543](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210125223048.png)

优点：

- 连接管理更简单
- 状态决策更简单

开源方案已经有比较成熟的中介式解决方案，例如 ZooKeeper 和 Keepalived。

3. 模拟式

模拟式指主备机之间并不传递任何状态数据，而是备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况来判断主机的状态。其基本架构如下：

![image-20210125223558337](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210125223558.png)

模拟式读写操作获取的状态信息只有响应信息（例如，HTTP 404，超时、响应时间超过 3 秒等），没有互连式那样多样（除了响应信息，还可以包含 CPU 负载、I/O 负载、吞吐量、响应时间等），基于有限的状态来做状态决策，可能出现偏差。

**主主复制**

主主复制指的是两台机器都是主机，互相将数据复制给对方，客户端可以任意挑选其中一台机器进行读写操作，基本架构图如下：

![image-20210125223947332](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210125223947.png)

主主复制架构事实上并不简单，如果采取主主复制架构，必须保证数据能够双向复制，而很多数据是不能双向复制的。例如：

- 用户注册后生成的用户 ID，如果按照数字增长，那就不能双向复制，否则就会出现 X 用户在主机 A 注册，分配的用户 ID 是 100，同时 Y 用户在主机 B 注册，分配的用户 ID 也是 100，这就出现了冲突。
- 库存不能双向复制。例如，一件商品库存 100 件，主机 A 上减了 1 件变成 99，主机 B 上减了 2 件变成 98，然后主机 A 将库存 99 复制到主机 B，主机 B 原有的库存 98 被覆盖，变成了 99，而实际上此时真正的库存是 97。类似的还有余额数据。

因此，主主复制架构对数据的设计有严格的要求，一般适合于那些临时性、可丢失、可覆盖的数据场景。例如，用户登录产生的 session 数据（可以重新登录生成）、用户行为的日志数据（可以丢失）、论坛的草稿数据（可以丢失）等。

**思考题**

如果你来设计一个政府信息公开网站的信息存储系统，你会采取哪种架构？谈谈你的分析和理由。

# 26 | 高可用存储架构：集群和分区

今天我们一起来看看另外两种常见的高可用存储架构：数据集群和数据分区。

简单来说，集群就是多台机器组合在一起形成一个统一的系统，这里的“多台”，数量上至少是 3 台；相比而言，主备、主从都是 2 台机器。根据集群中机器承担的不同角色来划分，集群可以分为两类：数据集中集群、数据分散集群。

**数据集中集群**

我们也可以称数据集中集群为 1 主多备或者 1 主多从。下图是读写全部到主机的一种架构：

![image-20210126222208246](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210126222208.png)

集群里面的服务器数量更多，复杂度整体也更高一些，具体体现在：

- 主机如何将数据复制给备机
- 备机如何检测主机状态
- 主机故障后，如何决定新的主机

目前开源的数据集中集群以 ZooKeeper 为典型，ZooKeeper 通过 ZAB 算法来解决上述提到的几个问题。

**数据分散集群**

数据分散集群指多个服务器组成一个集群，每台服务器都会负责存储一部分数据；同时，为了提升硬件利用率，每台服务器又会备份一部分数据。

数据分散集群的复杂点在于如何将数据分配到不同的服务器上，算法需要考虑这些设计点：

- 均衡性

  算法需要保证服务器上的数据分区基本是均衡的，不能存在某台服务器上的分区数量是另外一台服务器的几倍的情况。

- 容错性

  当出现部分服务器故障时，算法需要将原来分配给故障服务器的数据分区分配给其他服务器。

- 可伸缩性

  当集群容量不够，扩充新的服务器后，算法能够自动将部分数据分区迁移到新服务器，并保证扩容后所有服务器的均衡性。

数据分散集群和数据集中集群的不同点在于，数据分散集群中的每台服务器都可以处理读写请求，因此不存在数据集中集群中负责写的主机那样的角色。但在数据分散集群中，必须有一个角色来负责执行数据分配算法，这个角色可以是独立的一台服务器，也可以是集群自己选举出的一台服务器。

需要注意的是这里的“主机”和数据集中集群中的“主机”，其职责是有差异的。

Hadoop 的数据分区管理架构如下：

![image-20210126224130674](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210126224130.png)

与 Hadoop 不同的是，Elasticsearch 集群通过选举一台服务器来做数据分区的分配，叫作 master node，其数据分区管理架构是：

![image-20210126224632136](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210126224632.png)

一般来说，数据集中集群适合数据量不大，集群机器数量不多的场景。例如，ZooKeeper 集群，一般推荐 5 台机器左右；而数据分散集群，由于其良好的可伸缩性，适合业务数据量巨大、集群机器数量庞大的业务场景。例如，Hadoop 集群、HBase 集群，大规模的集群可以达到上百台甚至上千台服务器。

**数据分区**

前面我们讨论的存储高可用架构都是基于硬件故障的场景去考虑和设计的，但对于一些影响非常大的灾难或者事故来说，有可能所有的硬件全部故障。

这种情况下基于硬件故障而设计的高可用架构不再适用，我们需要基于地理级别的故障来设计高可用架构，这就是数据分区架构产生的背景。

数据分区指将数据按照一定的规则进行分区，不同分区分布在不同的地理位置上，每个分区存储一部分数据，通过这种方式来规避地理级别的故障所造成的巨大影响。

设计一个良好的数据分区架构，需要从多方面去考虑。

1. 数据量

数据量越大，分区规则会越复杂，考虑的情况也越多。

2. 分区规则

地理位置有近有远，因此可以得到不同的分区规则，包括洲际分区、国家分区、城市分区。

3. 复制规则

每个分区本身的数据量只是整体数据的一部分，这部分数据如果损坏或者丢失，损失同样难以接受。

因此即使是分区架构，同样需要考虑复制方案。常见的分区复制规则有三种：集中式、互备式和独立式。

**集中式**

集中式备份指存在一个总的备份中心，所有的分区都将数据备份到备份中心，其基本架构如下：

![image-20210126230406761](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210126230406.png)

优点：

- 设计简单，各分区之间并无直接联系，可以做到互不影响。
- 扩展容易，如果要增加第四个分区，例如武汉分区，只需要将武汉分区的数据复制到西安备份中心即可，其他分区不受影响。

缺点：

- 成本较高，需要建设一个独立的备份中心。

**互备式**

互备式备份指每个分区备份另外一个分区的数据，其基本架构如下：

![image-20210126230527750](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210126230527.png)

优点：

- 设计比较复杂，各个分区除了要承担业务数据存储，还需要承担备份功能，相互之间互相关联和影响。
- 扩展麻烦，如果增加一个武汉分区，则需要修改广州分区的复制指向武汉分区，然后将武汉分区的复制指向北京分区。而原有北京分区已经备份了的广州分区的数据怎么处理也是个难题。

缺点：

- 成本低，直接利用已有的设备。

**独立式**

独立式备份指每个分区自己有独立的备份中心，其基本架构如下：

![image-20210126230939538](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210126230939.png)

各个分区的备份并不和原来的分区在一个地方。这样做的主要目的是规避同城或者相同地理位置同时发生灾难性故障的极端情况。

优点：

- 设计简单，各分区互不影响。
- 扩展容易，新增加的分区只需要搭建自己的备份中心即可。

缺点：

- 成本高，每个分区需要独立的备份中心，备份中心的场地成本是主要成本，因此独立式比集中式成本要高很多。

# 27 | 如何设计计算高可用架构？

计算高可用架构设计的关键点有下面两点。

1. 哪些服务器可以执行任务

通常有两种类型，第一，每个服务器都可以执行任务；第二，只有主机可以执行任务。

2. 如何重新执行任务

通常有两种策略，第一种，对于已经分配的任务即使执行失败也不做任何处理，系统只需要保证新的任务能够分配到其他非故障服务器上执行即可；第二种，设计一个任务管理器，服务器执行完任务后，向任务管理器反馈任务执行结果，任务管理器根据结果来决定是否重新分配执行。

接下来，我将详细阐述常见的计算高可用架构：主备、主从和集群。

**主备**

计算高可用的主备架构无须数据复制，其基本的架构示意图如下：

![image-20210127224007528](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210127224113.png)

当主机故障时，如果能够恢复，任务分配器继续将任务发送给主机；如果不能恢复，则需要人工操作，将备机升为主机。同时，为了继续保持主备架构，需要人工增加新的机器作为备机。

根据备机状态的不同，主备架构又可以细分为冷备架构和温备架构。

> 冷备：备机上的程序包和配置文件都准备好，但备机上的业务系统没有启动。
>
> 温备：备机上的业务系统已经启动，只是不对外提供服务。

主备架构的优点是简单。缺点是切换需要人工操作，低效且容易出错。因此，计算高可用的主备架构比较适合与内部管理系统、后台管理系统这类使用人数不多、使用频率不高的业务，不太适合在线的业务。

**主从**

任务分配器需要将任务进行分类，确定哪些任务可以发送给主机执行，哪些任务可以发送给备机执行，其基本的架构示意图如下：

![image-20210127224648563](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210127224648.png)

当主机故障时，任务分配器还是会继续将任务发送给主机。如果主机能够恢复，任务分配器继续按照原有的设计策略分配任务；如果主机不能够恢复，则需要人工操作，将原来的从机升级为主机，并增加新的机器作为从机。

主从架构与主备架构相比，优缺点有：

- 优点：主从架构的从机也执行任务，发挥了从机的硬件性能。
- 缺点：主从架构需要将任务分类，任务分配器会复杂一些。

**集群**

在可用性要求更加严格的场景中，我们需要系统能够自动完成切换操作，这就是高可用集群方案。

高可用计算的集群方案根据集群中服务器节点角色的不同，可以分为两类：一类是对称集群，即集群中每个服务器的角色都是一样的，都可以执行所有任务；另一类是非对称集群，集群中的服务器分为多个不同的角色，分别执行不同的任务，例如最常见的 Master-Slave 角色。

1. 对称集群

对称集群更通俗的叫法是负载均衡集群，架构示意图如下：

![image-20210127230243616](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210127230243.png)

负载均衡集群详细设计：

- 正常情况下，任务分配器采取某种策略（随机、轮询等）将计算任务分配给集群中的不同服务器。
- 当集群中的某台服务器故障后，任务分配器不再将任务分配给它，而是将任务分配给其他服务器执行。
- 当故障的服务器恢复后，任务分配器重新将任务分配给它执行。

负载均衡集群的设计关键点在于两点：

- 任务分配器需要选取分配策略。
- 任务分配器需要检测服务器状态。

任务分配策略比较简单，轮询和随机基本就够了。状态检测稍微复杂一些，既要检测服务器的状态，同时还要检测任务的执行状态。常用的做法是任务分配器和服务器之间通过心跳来传递信息，然后根据实际情况来确定状态判断条件。

例如，一个在线页面访问系统，正常情况下页面平均会在 500 毫秒内返回，那么状态判断条件可以设计为：1 分钟内响应时间超过 1 秒（包括超时）的页面数量占了 80% 时，就认为服务器有故障。

2. 非对称集群

非对称集群中不同服务器的角色是不同的，不同角色的服务器承担不同的职责。非对称集群的基本架构示意图如下：

![image-20210127231147111](https://gitee.com/yanglu_u/ImgRepository/raw/master/images/20210127231147.png)

非对称集群架构详细设计：

- 集群会通过某种方式来区分不同服务器的角色。例如，通过 ZAB 算法选举，或者简单地取当前存活服务器中节点 ID 最小的服务器作为 Master 服务器。
- 任务分配器将不同任务发送给不同服务器。例如，图中的计算任务 A 发送给 Master 服务器，计算任务 B 发送给 Slave 服务器。
- 当指定类型的服务器故障时，需要重新分配角色。例如，Master 服务器故障后，需要将剩余的 Slave 服务器中的一个重新指定为 Master 服务器；如果是 Slave 服务器故障，则并不需要重新分配角色，只需要将故障服务器从集群剔除即可。

非对称集群相比负载均衡集群，设计复杂度主要体现在两个方面：

- 任务分配策略更加复杂：需要将任务划分为不同类型并分配给不同角色的集群节点。
- 角色分配策略实现比较复杂：例如，可能需要使用 ZAB、Raft 这类复杂的算法来实现 Leader 的选举。

我以 ZooKeeper 为例：

- 任务分配器：ZooKeeper 中不存在独立的任务分配器节点，每个 Server 都是任务分配器，Follower 收到请求后会进行判断，如果是写请求就转发给 Leader，如果是读请求就自己处理。
- 角色指定：ZooKeeper 通过 ZAB 算法来选举 Leader，当 Leader 故障后，所有的 Follower 节点会暂停读写操作，开始进行选举，直到新的 Leader 选举出来后才继续对 Client 提供服务。

# 28 | 业务高可用的保障：异地多活架构

在一些极端场景下，有可能所有服务器都出现故障。例如，典型的有机房断电、机房火灾、地震、水灾……如果业务期望达到即使在此类灾难性故障的情况下，业务也不受影响，或者在几分钟内就能够很快恢复，那么就需要设计异地多活架构。

**异地多活**

异地就是指地理位置上不同的地方；多活就是指不同地理位置上的系统都能够提供业务服务，这里的“活”是活动、活跃的意思。

与“活”对应的是字是“备”，备是备份，正常情况下对外是不提供服务的，如果需要提供服务，则需要大量的人工干预和操作，花费大量的时间才能让“备”变成“活”。

判断一个系统是否符合异地多活，需要满足两个标准：

- 正常情况下，用户无论访问哪一个地点的业务系统，都能够得到正确的业务服务。
- 某个地方业务异常的时候，用户访问其他地方正常的业务系统，能够得到正确的业务服务。

实现异地多活架构代价很高，具体表现为：

- 系统复杂度会发生质的变化，需要设计复杂的异地多活架构。
- 成本会上升，毕竟要多在一个或者多个机房搭建独立的一套业务系统。

**应用场景**

异地多活虽然功能很强大，但也不是每个业务不管三七二十一都要上异地多活。常见的新闻网站、企业内部的 IT 系统、游戏、博客站点等，如果无法承受异地多活带来的复杂度和成本，是可以不做异地多活的，只需要做异地备份即可。因为这类业务系统即使中断，对用户的影响并不会很大，例如，A 新闻网站看不了，用户换个新闻网站即可。

而共享单车、滴滴出行、支付宝、微信这类业务，就需要做异地多活了，这类业务系统中断后，对用户的影响很大。例如，支付宝用不了，就没法买东西了；滴滴用不了，用户就打不到车了。

**架构模式**

根据地理位置上的距离来划分，异地多活架构可以分为同城异区、跨城异地、跨国异地。

1. 同城异区

同城的两个机房，距离上一般大约就是几十千米，通过搭建高速的网络，同城异区的两个机房能够实现和同一个机房内几乎一样的网络传输速度。

同城异区能很好的解决机房火灾、机房停电、机房空调故障这类问题。

2. 跨城异地

跨城异地指的是业务部署在不同城市的多个机房，而且距离最好要远一些。跨城异地就是为了解决美加大停电、新奥尔良水灾这两类问题的。

跨城异地距离较远带来的网络传输延迟问题，例如，广州机房到北京机房，正常情况下 RTT 大约是 50 毫秒左右，遇到网络波动之类的情况，RTT 可能飙升到 500 毫秒甚至 1 秒，更不用说经常发生的线路丢包问题，那延迟可能就是几秒几十秒了。

如何解决这个问题呢？需要根据数据的特性来做不同的架构。如果是强一致性要求的数据，例如银行存款余额、支付宝余额等，这类数据实际上是无法做到跨城异地多活的。

而对数据一致性要求不那么高，或者数据不怎么改变，或者即使数据丢失影响也不大的业务，跨城异地多活就能够派上用场了。例如，用户登录（数据不一致时用户重新登录即可）、新闻类网站（一天内的新闻数据变化较少）、微博类网站（丢失用户发布的微博或者评论影响不大），这些业务采用跨城异地多活，能够很好地应对极端灾难的场景。

3. 跨国异地

跨国异地数据同步的延时会更长，正常情况下可能就有几秒钟。跨国异地多活的主要应用场景一般有这几种情况：

- 为不同地区用户提供服务

例如，亚马逊中国是为中国用户服务的，而亚马逊美国是为美国用户服务的，亚马逊中国的用户如果访问美国亚马逊，是无法用亚马逊中国的账号登录美国亚马逊的。

（这个是不是不符合异地多活的标准：访问任何一个地方的数据都能得到正确的业务服务？）

- 只读类业务做多活

例如，谷歌的搜索业务，由于用户搜索资料时，这些资料都已经存在于谷歌的搜索引擎上面，无论是访问英国谷歌，还是访问美国谷歌，搜索结果基本相同，并且对用户来说，也不需要搜索到最新的实时资料，跨国异地的几秒钟网络延迟，对搜索结果是没有什么影响的。

